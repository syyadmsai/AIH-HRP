{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98a0bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from joblib import dump\n",
    "import tweepy\n",
    "from joblib import load\n",
    "import shap\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298ce1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and vectorizer\n",
    "vectorizer = load(\"tfidf_vectorizer.joblib\")\n",
    "model = load(\"logistic_model.joblib\")\n",
    "\n",
    "# Define a prediction function that accepts raw text\n",
    "def model_predict(texts):\n",
    "    X = vectorizer.transform(texts)\n",
    "    return model.predict_proba(X)\n",
    "\n",
    "# SHAP explainer using callable function\n",
    "explainer = shap.Explainer(model_predict, vectorizer.transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7955b62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def explain_prediction_with_shap(text, user_id=None, show_plot=True):\n",
    "    \"\"\"\n",
    "    Predicts depression from user text and displays SHAP word contribution plot.\n",
    "    \"\"\"\n",
    "    pred_proba = model_predict([text])[0][1]\n",
    "    pred_label = int(pred_proba >= 0.5)\n",
    "\n",
    "    print(\"=\"*50)\n",
    "    print(f\"User: {user_id or 'Unknown'}\")\n",
    "    print(f\"Prediction: {'Depressed' if pred_label else 'Not Depressed'}\")\n",
    "    print(f\"Confidence: {pred_proba:.2f}\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    shap_values = explainer([text])\n",
    "    \n",
    "    if show_plot:\n",
    "        shap.plots.text(shap_values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b2388d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_texts_from_chunk(file_path):\n",
    "    \"\"\"Extracts all <TEXT>...</TEXT> sections from an individual chunk file.\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        content = f.read()\n",
    "    texts = re.findall(r'<TEXT>(.*?)</TEXT>', content, re.DOTALL)\n",
    "    texts = [text.strip().replace('\\n', ' ') for text in texts]\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93526028",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_erisk_chunked_data(user_data_path, truth_file_path, max_chunks=10, max_posts_per_chunk=None):\n",
    "    \"\"\"Loads user chunks and corresponding labels.\"\"\"\n",
    "    labels = {}\n",
    "    with open(truth_file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) == 2:\n",
    "                user_id_base, label = parts\n",
    "                labels[user_id_base] = int(label)\n",
    "\n",
    "    user_chunks = defaultdict(list)\n",
    "\n",
    "    for filename in os.listdir(user_data_path):\n",
    "        if filename.startswith(\"test_subject\"):\n",
    "            parts = filename.split(\"_\")\n",
    "            if len(parts) != 3:\n",
    "                continue\n",
    "\n",
    "            user_id_base = \"_\".join(parts[:2])  # e.g., test_subject25\n",
    "            chunk_idx = int(parts[2].split(\".\")[0])  # chunk number (1 to 10)\n",
    "\n",
    "            full_path = os.path.join(user_data_path, filename)\n",
    "            posts = extract_texts_from_chunk(full_path)\n",
    "\n",
    "            if max_posts_per_chunk:\n",
    "                posts = posts[:max_posts_per_chunk]\n",
    "\n",
    "            chunk_text = \" \".join(posts)\n",
    "            user_chunks[user_id_base].append((chunk_idx, chunk_text))\n",
    "    \n",
    "    data = []\n",
    "    for user_id_base, chunks in user_chunks.items():\n",
    "        sorted_chunks = sorted(chunks, key=lambda x: x[0])\n",
    "        sorted_chunks = sorted_chunks[:max_chunks]\n",
    "        chunk_texts = [text for idx, text in sorted_chunks]\n",
    "\n",
    "        label = labels.get(user_id_base, None)\n",
    "        if label is not None:\n",
    "            data.append({\n",
    "                \"user_id\": user_id_base,\n",
    "                \"chunks\": chunk_texts,\n",
    "                \"label\": label\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4152d596",
   "metadata": {},
   "outputs": [],
   "source": [
    "def progressive_training(train_df, test_df, chunk_stages=[2, 4, 6, 8, 10]):\n",
    "    \"\"\"Train and evaluate progressively as more chunks are revealed.\"\"\"\n",
    "    results = {}\n",
    "\n",
    "    for n_chunks in chunk_stages:\n",
    "        print(f\"==== Training with first {n_chunks} chunks ====\")\n",
    "\n",
    "        # Prepare data\n",
    "        X_train = train_df['chunks'].apply(lambda chunks: \" \".join(chunks[:n_chunks]))\n",
    "        y_train = train_df['label']\n",
    "\n",
    "        X_test = test_df['chunks'].apply(lambda chunks: \" \".join(chunks[:n_chunks]))\n",
    "        y_test = test_df['label']\n",
    "\n",
    "        # TF-IDF\n",
    "        vectorizer = TfidfVectorizer(max_features=5000)\n",
    "        X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "        X_test_tfidf = vectorizer.transform(X_test)\n",
    "        dump(vectorizer, \"tfidf_vectorizer.joblib\")\n",
    "        \n",
    "        # Train\n",
    "        model = LogisticRegression(max_iter=1000, class_weight='balanced')\n",
    "        model.fit(X_train_tfidf, y_train)\n",
    "        dump(model, \"logistic_model.joblib\")\n",
    "        # Predict\n",
    "        y_pred = model.predict(X_test_tfidf)\n",
    "\n",
    "        # Metrics\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        prec = precision_score(y_test, y_pred, zero_division=0)\n",
    "        rec = recall_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "        results[n_chunks] = {\n",
    "            'accuracy': acc,\n",
    "            'precision': prec,\n",
    "            'recall': rec,\n",
    "            'f1': f1\n",
    "        }\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cab057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“ Set paths\n",
    "train_data_path = r'C:\\Users\\SYYAD\\Documents\\MSAI\\AI in Healthcare\\HRP\\eRisk2017\\2017\\test\\user_data'\n",
    "train_truth_path = r'C:\\Users\\SYYAD\\Documents\\MSAI\\AI in Healthcare\\HRP\\eRisk2017\\2017\\test\\test_golden_truth.txt'\n",
    "\n",
    "# Load data\n",
    "train_df = load_erisk_chunked_data(train_data_path, train_truth_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b673363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into Train and Test\n",
    "def split_train_test(df, test_size=0.2, random_state=42):\n",
    "    shuffled_df = df.sample(frac=1, random_state=random_state)\n",
    "    split_idx = int(len(shuffled_df) * (1 - test_size))\n",
    "    train = shuffled_df.iloc[:split_idx]\n",
    "    test = shuffled_df.iloc[split_idx:]\n",
    "    return train, test\n",
    "\n",
    "train_users, test_users = split_train_test(train_df)\n",
    "\n",
    "# Train and Evaluate\n",
    "results = progressive_training(train_users, test_users)\n",
    "\n",
    "# ðŸ“‹ Print results nicely\n",
    "print(\"Chunks | Accuracy | Precision | Recall | F1\")\n",
    "print(\"----------------------------------------------\")\n",
    "for n_chunks, metrics in sorted(results.items()):\n",
    "    print(f\"{n_chunks:>6} | {metrics['accuracy']:.3f}   | {metrics['precision']:.3f}    | {metrics['recall']:.3f}  | {metrics['f1']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3769c845",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_numbers = []\n",
    "accuracies = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1s = []\n",
    "\n",
    "for n_chunks, metrics in results.items():\n",
    "    chunk_numbers.append(n_chunks)\n",
    "    accuracies.append(metrics['accuracy'])\n",
    "    precisions.append(metrics['precision'])\n",
    "    recalls.append(metrics['recall'])\n",
    "    f1s.append(metrics['f1'])\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(chunk_numbers, accuracies, marker='o', label='Accuracy')\n",
    "plt.plot(chunk_numbers, precisions, marker='o', label='Precision')\n",
    "plt.plot(chunk_numbers, recalls, marker='o', label='Recall')\n",
    "plt.plot(chunk_numbers, f1s, marker='o', label='F1 Score')\n",
    "\n",
    "plt.title('Progressive Depression Detection Performance')\n",
    "plt.xlabel('Number of Chunks Seen')\n",
    "plt.ylabel('Score')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67df401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Twitter API keys and tokens\n",
    "bearer_token = \"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\" # Replace with your actual bearer token\n",
    "\n",
    "client = tweepy.Client(bearer_token=bearer_token)\n",
    "\n",
    "def get_last_10_tweets(username):\n",
    "    user = client.get_user(username=username)\n",
    "    tweets = client.get_users_tweets(user.data.id, max_results=5, exclude=[\"replies\", \"retweets\"])\n",
    "    return [tweet.text for tweet in tweets.data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ee5bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and vectorizer\n",
    "vectorizer = load(\"tfidf_vectorizer.joblib\")\n",
    "model = load(\"logistic_model.joblib\")\n",
    "\n",
    "# Clean function\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text)  # remove URLs\n",
    "    text = re.sub(r'\\@\\w+|\\#','', text)  # remove mentions and hashtags\n",
    "    return text.lower()\n",
    "\n",
    "# Model prediction function that accepts raw text and returns probabilities\n",
    "def predict_proba(texts):\n",
    "    cleaned = [clean_text(t) for t in texts]\n",
    "    features = vectorizer.transform(cleaned)\n",
    "    return model.predict_proba(features)\n",
    "\n",
    "# Create a SHAP explainer for raw text input\n",
    "text_explainer = shap.Explainer(predict_proba, shap.maskers.Text(r\"\\W+\"))\n",
    "\n",
    "# Full prediction and explanation function\n",
    "def predict_depression_with_explanation(tweets, show_plot=True):\n",
    "    combined = \" \".join([clean_text(t) for t in tweets])\n",
    "    pred = model.predict(vectorizer.transform([combined]))[0]\n",
    "    label = \"Depressed\" if pred else \"Not Depressed\"\n",
    "    prob = model.predict_proba(vectorizer.transform([combined]))[0][1]\n",
    "\n",
    "    print(f\"Prediction: {label}\")\n",
    "    print(f\"Confidence: {prob:.2f}\")\n",
    "\n",
    "    if show_plot:\n",
    "        shap_values = text_explainer([combined])\n",
    "        shap.plots.text(shap_values[0])\n",
    "\n",
    "    return label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "381674d3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_last_10_tweets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m tweets \u001b[38;5;241m=\u001b[39m \u001b[43mget_last_10_tweets\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthisusertwtss\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLast 5 tweets from elonmusk: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtweets\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'get_last_10_tweets' is not defined"
     ]
    }
   ],
   "source": [
    "tweets = get_last_10_tweets(\"thisusertwtss\")\n",
    "print(f\"Last 5 tweets from elonmusk: {tweets}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e26c916",
   "metadata": {},
   "outputs": [],
   "source": [
    "status = predict_depression_with_explanation(tweets)\n",
    "print(f\"Predicted status: {status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e11a75d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_last_10_tweets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m tweets \u001b[38;5;241m=\u001b[39m \u001b[43mget_last_10_tweets\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthisusertwtss\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLast 5 tweets from thisusertwtss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtweets\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'get_last_10_tweets' is not defined"
     ]
    }
   ],
   "source": [
    "tweets = get_last_10_tweets(\"thisusertwtss\")\n",
    "print(f\"Last 5 tweets from thisusertwtss: {tweets}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daab45bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "status = predict_depression_with_explanation(tweets)\n",
    "print(f\"Predicted status: {status}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_hw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
